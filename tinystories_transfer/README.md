# Multilingual Language Model Training Framework

A comprehensive framework for training small-scale language models in multiple languages using PyTorch Lightning and Hugging Face Transformers. This project provides a complete pipeline for preprocessing data, building tokenizers, tokenizing datasets, and training GPTNeo language models with performance evaluation.

## Features

- ğŸŒ **Multilingual Support**: Train models on different languages (currently English and Czech)
- ğŸ”„ **Complete Pipeline**: From raw text preprocessing to model training and evaluation
- ğŸš… **High-Performance Training**: PyTorch Lightning for efficient, distributed training
- ğŸ“Š **Integrated Monitoring**: WandB integration for experiment tracking
- ğŸ“ **Text Generation**: Automatic text generation samples during training
- ğŸ“ **Perplexity Evaluation**: Built-in perplexity calculation for model assessment
- ğŸ–¥ï¸ **HPC Support**: Ready-to-use scripts for high-performance computing environments

## Project Structure

```
â”œâ”€â”€ configs/
â”‚   â””â”€â”€ config.yaml           # Hydra configuration file
â”œâ”€â”€ data/                     # Data directory (gitignored)
â”‚   â”œâ”€â”€ en/                   # English dataset
â”‚   â””â”€â”€ cs/                   # Czech dataset
â”œâ”€â”€ hpc_scripts/              # Slurm scripts for HPC environments
â”‚   â”œâ”€â”€ preprocess.sh         # Preprocessing script
â”‚   â”œâ”€â”€ tokenizer.sh          # Tokenizer creation script
â”‚   â”œâ”€â”€ tokenize_data.sh      # Data tokenization script
â”‚   â””â”€â”€ train.sh              # Training script
â”œâ”€â”€ models/                   # Saved models directory (gitignored)
â”œâ”€â”€ tokenizers/               # Saved tokenizers directory
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ preprocess.py         # Data preprocessing utility
â”‚   â”œâ”€â”€ tokenizer.py          # Tokenizer creation utility
â”‚   â””â”€â”€ tokenize_data.py      # Data tokenization utility
â”œâ”€â”€ train.py                  # Main training script
â”œâ”€â”€ ppl.py                    # Perplexity calculation implementation
â”œâ”€â”€ requirements.txt          # Project dependencies
â””â”€â”€ .gitignore                # Git ignore file
```

## Installation

1. Clone the repository:
```bash
git clone https://github.com/pe-hy/tinystories_transfer.git
cd tinystories_transfer
```

2. Create an environment and install dependencies:
```bash
pip install -r requirements.txt
```

### Data Preparation

1. Organize your text data in the following structure:
```
data/
â”œâ”€â”€ en/
â”‚   â”œâ”€â”€ train.txt
â”‚   â””â”€â”€ val.txt
â””â”€â”€ cs/
    â”œâ”€â”€ train.txt
    â””â”€â”€ val.txt
```

2. Preprocess the data:
```bash
python utils/preprocess.py
```

### Tokenizer Creation

Create BPE tokenizers for your languages:
```bash
python utils/tokenizer.py
```

### Data Tokenization

Tokenize your text data with the created tokenizers:
```bash
python utils/tokenize_data.py
```

### Model Training

Train the model:
```bash
python train.py
```

### HPC Usage

For high-performance computing environments with Slurm:

1. Preprocessing:
```bash
sbatch hpc_scripts/preprocess.sh
```

2. Create tokenizers:
```bash
sbatch hpc_scripts/tokenizer.sh
```

3. Tokenize data:
```bash
sbatch hpc_scripts/tokenize_data.sh
```

4. Training:
```bash
sbatch hpc_scripts/train.sh
```

## Configuration

The project uses Hydra for configuration management. Key configuration parameters in `configs/config.yaml`:

### Data Configuration
```yaml
data:
  datapath: "data"
  languages: ["en", "cs"]
  # ... more options
```

### Model Configuration
```yaml
model:
  batch_size: 80
  accumulate_grad_batches: 16
  block_size: 512
  epochs: 50
  n_head: 16
  hidden_size: 768
  # ... more options
```

### Tokenizer Configuration
```yaml
tokenizer:
  vocab_size: 30000
  # ... more options
```

## Features in Detail

### Text Generation During Training

The model automatically generates sample texts during training based on prompts in configured languages. These samples are logged to WandB for qualitative assessment.

### Perplexity Evaluation

The framework includes comprehensive perplexity calculation for model evaluation, allowing you to assess language model quality beyond just loss values.

### Weights & Biases Integration

Training progress, hyperparameters, generated text samples, and evaluation metrics are logged to Weights & Biases for easy experiment tracking.

## Customization

### Adding New Languages

To add a new language:
1. Add the language code to the `languages` list in `configs/config.yaml`
2. Create data folders for the new language
3. Run the preprocessing, tokenizer creation, and data tokenization steps

### Modifying Model Architecture

Model architecture parameters can be adjusted in the `model` section of the configuration file:
```yaml
model:
  n_head: 16          # Number of attention heads
  hidden_size: 768    # Hidden size dimension
  n_layer: 4          # Number of transformer layers
  # ... more options
```

## License

This project is licensed under the MIT License - see the LICENSE file for details.

## Acknowledgements

This project leverages several powerful libraries:
- [PyTorch Lightning](https://www.pytorchlightning.ai/)
- [Hugging Face Transformers](https://huggingface.co/transformers/)
- [Hydra](https://hydra.cc/)
- [Weights & Biases](https://wandb.ai/)
